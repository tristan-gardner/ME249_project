{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.990099\n",
      "1    0.990099\n",
      "2    0.990099\n",
      "3    1.000000\n",
      "4    0.990099\n",
      "5    1.000000\n",
      "6    1.188119\n",
      "7    1.782178\n",
      "Name: x01, dtype: float64 0    0.896552\n",
      "1    1.000000\n",
      "2    1.055172\n",
      "3    0.896552\n",
      "4    1.000000\n",
      "5    1.055172\n",
      "6    0.896552\n",
      "7    1.000000\n",
      "Name: x02, dtype: float64 0    1.009091\n",
      "1    1.000000\n",
      "2    0.993506\n",
      "3    1.009091\n",
      "4    1.000000\n",
      "5    0.993506\n",
      "6    1.009091\n",
      "7    1.000325\n",
      "Name: x03, dtype: float64 0    0.956452\n",
      "1    0.993796\n",
      "2    0.978365\n",
      "3    0.954292\n",
      "4    0.999969\n",
      "5    0.969106\n",
      "6    1.096571\n",
      "7    1.432055\n",
      "Name: y3, dtype: float64\n",
      "[[0.9900990099009901, 0.896551724137931, 1.009090909090909], [0.9900990099009901, 1.0, 1.0], [0.9900990099009901, 1.0551724137931036, 0.9935064935064936], [1.0, 0.896551724137931, 1.009090909090909], [0.9900990099009901, 1.0, 1.0], [1.0, 1.0551724137931036, 0.9935064935064936], [1.188118811881188, 0.896551724137931, 1.009090909090909], [1.7821782178217822, 1.0, 1.0003246753246755]]\n",
      "[[0.99009901 0.89655172 1.00909091]\n",
      " [0.99009901 1.         1.        ]\n",
      " [0.99009901 1.05517241 0.99350649]\n",
      " [1.         0.89655172 1.00909091]\n",
      " [0.99009901 1.         1.        ]\n",
      " [1.         1.05517241 0.99350649]\n",
      " [1.18811881 0.89655172 1.00909091]\n",
      " [1.78217822 1.         1.00032468]]\n"
     ]
    }
   ],
   "source": [
    "'''>>>>> start CodeP2.2F24\n",
    "    V.P. Carey ME249, Fall 2024\n",
    "\n",
    "Intro to Neural Network Modeling \n",
    "Keras model for comparison with first principles model'''\n",
    "\n",
    "#import useful packages\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import keras.backend as kb\n",
    "import tensorflow as tf\n",
    "#the follwoing 2 lines are only needed for Mac OS machines\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "#raw data in dictionary form x01, x02, x03, y3\n",
    "my_dict = { \n",
    "    'x01' : [20., 20., 20., 20.2, 20., 20.2, 24.0, 36.],\n",
    "    'x02' : [13., 14.5, 15.3, 13., 14.5, 15.3, 13., 14.5],\n",
    "    'x03' : [310.8, 308.0, 306.0, 310.8, 308.0, 306.0, 310.8, 308.1],\n",
    "    'y3' : [30.99, 32.2, 31.7, 30.92, 32.4, 31.4, 35.53, 46.4]\n",
    "}\n",
    "#normalized inputs in array\n",
    "xdata = []\n",
    "xdata = [[20./20.2, 13.0/14.5, 310.8/308.0], [20./20.2, 14.5/14.5, 308.0/308.0]] \n",
    "xdata.append([20./20.2, 15.3/14.5, 306.0/308.0])\n",
    "xdata.append([20.2/20.2, 13.0/14.5, 310.8/308.0]) \n",
    "xdata.append([20./20.2, 14.5/14.5, 308.0/308.0]) \n",
    "xdata.append([20.2/20.2, 15.3/14.5, 306.0/308.0]) \n",
    "xdata.append([24./20.2, 13.0/14.5, 310.8/308.0]) \n",
    "xdata.append([36./20.2, 14.5/14.5, 308.1/308.0]) \n",
    "\n",
    "#data frame\n",
    "df = pd.DataFrame(my_dict)\n",
    "#devide by the median to normalize \n",
    "df.x01= df.x01/20.2\n",
    "df.x02= df.x02/14.5\n",
    "df.x03= df.x03/308.0\n",
    "#normalize output array\n",
    "df.y3= df.y3/32.401\n",
    "df.head\n",
    "print (df.x01, df.x02, df.x03, df.y3)\n",
    "\n",
    "xarray= np.array(xdata)\n",
    "print (xdata)\n",
    "print (xarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_one (Dense)           (None, 1)                 4         \n",
      "                                                                 \n",
      " dense_two (Dense)           (None, 1)                 2         \n",
      "                                                                 \n",
      " dense_three (Dense)         (None, 1)                 2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8\n",
      "Trainable params: 8\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(3, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/python3-10_keras_env/lib/python3.10/site-packages/keras/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "\n",
    "#As seen below, we have created three dense layers each with just one neuron. \n",
    "#A dense layer is a layer in neural network that’s fully connected. \n",
    "#In other words, all the neurons in one layer are connected to all other neurons in the next layer.\n",
    "#In the first layer, we need to provide the input shape, which is 3 in this case. \n",
    "#The activation function we have chosen is ReLU, which stands for rectified linear unit.\n",
    "\n",
    "from keras import backend as K\n",
    "#initialize weights with values between -0.2 and 1.2\n",
    "initializer = keras.initializers.RandomUniform(minval= -0.2, maxval=1.2)\n",
    "\n",
    "# define three layer model with one neuron in each layer\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(1, activation=K.elu, input_shape=[3],  kernel_initializer=initializer, name=\"dense_one\"),\n",
    "    keras.layers.Dense(1, activation=K.elu,  kernel_initializer=initializer, name=\"dense_two\"),\n",
    "    keras.layers.Dense(1, activation=K.elu,  kernel_initializer=initializer, name=\"dense_three\")\n",
    "  ])\n",
    "model.summary()\n",
    "\n",
    "mult = 1.2\n",
    "\n",
    "#set starting values to those used in first principles model\n",
    "w01n =  1.23 * mult\n",
    "w02n =  0.40 * mult\n",
    "w03n =  0.70 * mult\n",
    "b1n =  -0.15 * mult\n",
    "w12n =  0.72 * mult\n",
    "b2n =  -0.12 * mult\n",
    "w23n =  0.7 * mult\n",
    "b3n =  0.01 * mult\n",
    "\n",
    "weights0 =  [[ w01n], [w02n], [ w03n]]\n",
    "w0array= np.array(weights0)\n",
    "print(np.shape(w0array))\n",
    "bias0 = [b1n]\n",
    "bias0array= np.array(bias0)\n",
    "L0=[]\n",
    "L0.append(w0array)\n",
    "L0.append(bias0array)\n",
    "model.layers[0].set_weights(L0) \n",
    "\n",
    "weights1 =  [[ w12n]]\n",
    "w1array= np.array(weights1)\n",
    "print(np.shape(w1array))\n",
    "bias1 = [b2n]\n",
    "bias1array= np.array(bias1)\n",
    "L1=[]\n",
    "L1.append(w1array)\n",
    "L1.append(bias1array)\n",
    "model.layers[1].set_weights(L1)\n",
    "\n",
    "weights2 =  [[ w23n]]\n",
    "w2array= np.array(weights2)\n",
    "print(np.shape(w2array))\n",
    "bias2 = [b3n]\n",
    "bias2array= np.array(bias2)\n",
    "L2=[]\n",
    "L2.append(w2array)\n",
    "L2.append(bias2array)\n",
    "model.layers[2].set_weights(L2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We’re using RMSprop as our optimizer here. RMSprop stands for Root Mean Square Propagation. \n",
    "#It’s one of the most popular gradient descent optimization algorithms for deep learning networks. \n",
    "#RMSprop is an optimizer that’s reliable and fast.\n",
    "#We’re compiling the mode using the model.compile function. The loss function used here \n",
    "#is mean absolute error. After the compilation of the model, we’ll use the fit method with 100 epochs.\n",
    "\n",
    "#Running model.fit successive times extends the calculation to addtional epochs.\n",
    "\n",
    "rms = keras.optimizers.RMSprop(0.0035)\n",
    "model.compile(loss='mean_absolute_error',optimizer=rms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.8591\n",
      "Epoch 2/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7564\n",
      "Epoch 3/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6843"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 17:51:10.024178: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6843\n",
      "Epoch 4/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6255\n",
      "Epoch 5/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5743\n",
      "Epoch 6/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5282\n",
      "Epoch 7/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4857\n",
      "Epoch 8/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4461\n",
      "Epoch 9/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4087\n",
      "Epoch 10/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3732\n",
      "Epoch 11/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3391\n",
      "Epoch 12/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3063\n",
      "Epoch 13/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2746\n",
      "Epoch 14/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2439\n",
      "Epoch 15/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2141\n",
      "Epoch 16/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1850\n",
      "Epoch 17/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1565\n",
      "Epoch 18/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1287\n",
      "Epoch 19/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1014\n",
      "Epoch 20/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0747\n",
      "Epoch 21/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0484\n",
      "Epoch 22/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0329\n",
      "Epoch 23/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0307\n",
      "Epoch 24/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0304\n",
      "Epoch 25/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0303\n",
      "Epoch 26/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0302\n",
      "Epoch 27/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0301\n",
      "Epoch 28/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0308\n",
      "Epoch 29/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0308\n",
      "Epoch 30/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0330\n",
      "Epoch 31/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0321\n",
      "Epoch 32/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0318\n",
      "Epoch 33/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0334\n",
      "Epoch 34/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0309\n",
      "Epoch 35/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0306\n",
      "Epoch 36/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0341\n",
      "Epoch 37/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0320\n",
      "Epoch 38/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0325\n",
      "Epoch 39/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0335\n",
      "Epoch 40/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0310\n",
      "Epoch 41/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0303\n",
      "Epoch 42/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0350\n",
      "Epoch 43/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0318\n",
      "Epoch 44/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0331\n",
      "Epoch 45/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0333\n",
      "Epoch 46/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0312\n",
      "Epoch 47/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0352\n",
      "Epoch 48/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0347\n",
      "Epoch 49/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0313\n",
      "Epoch 50/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0328\n",
      "Epoch 51/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0328\n",
      "Epoch 52/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0309\n",
      "Epoch 53/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0346\n",
      "Epoch 54/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0344\n",
      "Epoch 55/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0308\n",
      "Epoch 56/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0325\n",
      "Epoch 57/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0324\n",
      "Epoch 58/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0306\n",
      "Epoch 59/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0341\n",
      "Epoch 60/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0341\n",
      "Epoch 61/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0304\n",
      "Epoch 62/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0322\n",
      "Epoch 63/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0319\n",
      "Epoch 64/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0303\n",
      "Epoch 65/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0335\n",
      "Epoch 66/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0338\n",
      "Epoch 67/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0300\n",
      "Epoch 68/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0320\n",
      "Epoch 69/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0315\n",
      "Epoch 70/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0301\n",
      "Epoch 71/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0330\n",
      "Epoch 72/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0284\n",
      "Epoch 73/400\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0296\n",
      "Epoch 74/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0325\n",
      "Epoch 75/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0312\n",
      "Epoch 76/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0304\n",
      "Epoch 77/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0327\n",
      "Epoch 78/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0283\n",
      "Epoch 79/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0351\n",
      "Epoch 80/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0318\n",
      "Epoch 81/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0306\n",
      "Epoch 82/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0298\n",
      "Epoch 83/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0321\n",
      "Epoch 84/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0279\n",
      "Epoch 85/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0343\n",
      "Epoch 86/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0313\n",
      "Epoch 87/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0301\n",
      "Epoch 88/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0294\n",
      "Epoch 89/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0316\n",
      "Epoch 90/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0275\n",
      "Epoch 91/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0337\n",
      "Epoch 92/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0310\n",
      "Epoch 93/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0296\n",
      "Epoch 94/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0291\n",
      "Epoch 95/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0311\n",
      "Epoch 96/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0273\n",
      "Epoch 97/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0331\n",
      "Epoch 98/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0307\n",
      "Epoch 99/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0291\n",
      "Epoch 100/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0289\n",
      "Epoch 101/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0307\n",
      "Epoch 102/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0270\n",
      "Epoch 103/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0325\n",
      "Epoch 104/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0305\n",
      "Epoch 105/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0287\n",
      "Epoch 106/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0286\n",
      "Epoch 107/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0302\n",
      "Epoch 108/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0268\n",
      "Epoch 109/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0319\n",
      "Epoch 110/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0302\n",
      "Epoch 111/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0282\n",
      "Epoch 112/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0284\n",
      "Epoch 113/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0297\n",
      "Epoch 114/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0266\n",
      "Epoch 115/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0313\n",
      "Epoch 116/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0300\n",
      "Epoch 117/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0278\n",
      "Epoch 118/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0282\n",
      "Epoch 119/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0293\n",
      "Epoch 120/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0264\n",
      "Epoch 121/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0308\n",
      "Epoch 122/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0248\n",
      "Epoch 123/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0274\n",
      "Epoch 124/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0288\n",
      "Epoch 125/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0289\n",
      "Epoch 126/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0268\n",
      "Epoch 127/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0304\n",
      "Epoch 128/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0248\n",
      "Epoch 129/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0327\n",
      "Epoch 130/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0282\n",
      "Epoch 131/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0283\n",
      "Epoch 132/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0263\n",
      "Epoch 133/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0298\n",
      "Epoch 134/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0244\n",
      "Epoch 135/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0319\n",
      "Epoch 136/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0279\n",
      "Epoch 137/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0277\n",
      "Epoch 138/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0260\n",
      "Epoch 139/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0292\n",
      "Epoch 140/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0242\n",
      "Epoch 141/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0311\n",
      "Epoch 142/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0276\n",
      "Epoch 143/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0272\n",
      "Epoch 144/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0258\n",
      "Epoch 145/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0287\n",
      "Epoch 146/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0240\n",
      "Epoch 147/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0304\n",
      "Epoch 148/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0274\n",
      "Epoch 149/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0267\n",
      "Epoch 150/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0257\n",
      "Epoch 151/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0282\n",
      "Epoch 152/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0238\n",
      "Epoch 153/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0298\n",
      "Epoch 154/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0273\n",
      "Epoch 155/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0262\n",
      "Epoch 156/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0255\n",
      "Epoch 157/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0277\n",
      "Epoch 158/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0237\n",
      "Epoch 159/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0291\n",
      "Epoch 160/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0226\n",
      "Epoch 161/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0226\n",
      "Epoch 162/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0225\n",
      "Epoch 163/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0224\n",
      "Epoch 164/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0227\n",
      "Epoch 165/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0250\n",
      "Epoch 166/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0309\n",
      "Epoch 167/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0284\n",
      "Epoch 168/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0258\n",
      "Epoch 169/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0261\n",
      "Epoch 170/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0273\n",
      "Epoch 171/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0239\n",
      "Epoch 172/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0289\n",
      "Epoch 173/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0274\n",
      "Epoch 174/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0249\n",
      "Epoch 175/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0254\n",
      "Epoch 176/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0264\n",
      "Epoch 177/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0235\n",
      "Epoch 178/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0279\n",
      "Epoch 179/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0221\n",
      "Epoch 180/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0240\n",
      "Epoch 181/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0264\n",
      "Epoch 182/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0255\n",
      "Epoch 183/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0243\n",
      "Epoch 184/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0270\n",
      "Epoch 185/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0224\n",
      "Epoch 186/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0229\n",
      "Epoch 187/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0273\n",
      "Epoch 188/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0244\n",
      "Epoch 189/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0251\n",
      "Epoch 190/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0259\n",
      "Epoch 191/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0230\n",
      "Epoch 192/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0274\n",
      "Epoch 193/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0217\n",
      "Epoch 194/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0233\n",
      "Epoch 195/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0260\n",
      "Epoch 196/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0248\n",
      "Epoch 197/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0238\n",
      "Epoch 198/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0263\n",
      "Epoch 199/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0220\n",
      "Epoch 200/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0222\n",
      "Epoch 201/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0267\n",
      "Epoch 202/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0237\n",
      "Epoch 203/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0245\n",
      "Epoch 204/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0252\n",
      "Epoch 205/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0224\n",
      "Epoch 206/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0266\n",
      "Epoch 207/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0212\n",
      "Epoch 208/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0225\n",
      "Epoch 209/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0253\n",
      "Epoch 210/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0240\n",
      "Epoch 211/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0231\n",
      "Epoch 212/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0255\n",
      "Epoch 213/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0216\n",
      "Epoch 214/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0213\n",
      "Epoch 215/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0261\n",
      "Epoch 216/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0228\n",
      "Epoch 217/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0239\n",
      "Epoch 218/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0243\n",
      "Epoch 219/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0219\n",
      "Epoch 220/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0204\n",
      "Epoch 221/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0212\n",
      "Epoch 222/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0216\n",
      "Epoch 223/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0257\n",
      "Epoch 224/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0231\n",
      "Epoch 225/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0233\n",
      "Epoch 226/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0246\n",
      "Epoch 227/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0215\n",
      "Epoch 228/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0202\n",
      "Epoch 229/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0207\n",
      "Epoch 230/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0218\n",
      "Epoch 231/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0250\n",
      "Epoch 232/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0232\n",
      "Epoch 233/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0225\n",
      "Epoch 234/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0247\n",
      "Epoch 235/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0210\n",
      "Epoch 236/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0202\n",
      "Epoch 237/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0255\n",
      "Epoch 238/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0217\n",
      "Epoch 239/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0231\n",
      "Epoch 240/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0232\n",
      "Epoch 241/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0213\n",
      "Epoch 242/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0193\n",
      "Epoch 243/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0205\n",
      "Epoch 244/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0204\n",
      "Epoch 245/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0249\n",
      "Epoch 246/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0218\n",
      "Epoch 247/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0224\n",
      "Epoch 248/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0233\n",
      "Epoch 249/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0208\n",
      "Epoch 250/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0191\n",
      "Epoch 251/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0201\n",
      "Epoch 252/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0204\n",
      "Epoch 253/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0241\n",
      "Epoch 254/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0219\n",
      "Epoch 255/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0216\n",
      "Epoch 256/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0184\n",
      "Epoch 257/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0185\n",
      "Epoch 258/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0204\n",
      "Epoch 259/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0197\n",
      "Epoch 260/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0252\n",
      "Epoch 261/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0211\n",
      "Epoch 262/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0224\n",
      "Epoch 263/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0225\n",
      "Epoch 264/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0206\n",
      "Epoch 265/400\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0183\n",
      "Epoch 266/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0198\n",
      "Epoch 267/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0194\n",
      "Epoch 268/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0240\n",
      "Epoch 269/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0208\n",
      "Epoch 270/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0213\n",
      "Epoch 271/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0178\n",
      "Epoch 272/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0177\n",
      "Epoch 273/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0180\n",
      "Epoch 274/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0198\n",
      "Epoch 275/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0195\n",
      "Epoch 276/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0244\n",
      "Epoch 277/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0207\n",
      "Epoch 278/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0214\n",
      "Epoch 279/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0221\n",
      "Epoch 280/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0199\n",
      "Epoch 281/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0176\n",
      "Epoch 282/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0191\n",
      "Epoch 283/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0189\n",
      "Epoch 284/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0229\n",
      "Epoch 285/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0203\n",
      "Epoch 286/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0206\n",
      "Epoch 287/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0171\n",
      "Epoch 288/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0170\n",
      "Epoch 289/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0194\n",
      "Epoch 290/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0180\n",
      "Epoch 291/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0240\n",
      "Epoch 292/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0247\n",
      "Epoch 293/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0204\n",
      "Epoch 294/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0169\n",
      "Epoch 295/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0168\n",
      "Epoch 296/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0167\n",
      "Epoch 297/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0169\n",
      "Epoch 298/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0187\n",
      "Epoch 299/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0182\n",
      "Epoch 300/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0224\n",
      "Epoch 301/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0195\n",
      "Epoch 302/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0202\n",
      "Epoch 303/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0164\n",
      "Epoch 304/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0163\n",
      "Epoch 305/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0166\n",
      "Epoch 306/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0186\n",
      "Epoch 307/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0180\n",
      "Epoch 308/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0229\n",
      "Epoch 309/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0250\n",
      "Epoch 310/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0196\n",
      "Epoch 311/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0161\n",
      "Epoch 312/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0160\n",
      "Epoch 313/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0160\n",
      "Epoch 314/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0182\n",
      "Epoch 315/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0170\n",
      "Epoch 316/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0215\n",
      "Epoch 317/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0184\n",
      "Epoch 318/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0197\n",
      "Epoch 319/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0158\n",
      "Epoch 320/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0157\n",
      "Epoch 321/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0157\n",
      "Epoch 322/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0181\n",
      "Epoch 323/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0169\n",
      "Epoch 324/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0223\n",
      "Epoch 325/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0243\n",
      "Epoch 326/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0243\n",
      "Epoch 327/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0171\n",
      "Epoch 328/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0182\n",
      "Epoch 329/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0154\n",
      "Epoch 330/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0154\n",
      "Epoch 331/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0153\n",
      "Epoch 332/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0170\n",
      "Epoch 333/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0161\n",
      "Epoch 334/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0197\n",
      "Epoch 335/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0155\n",
      "Epoch 336/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0187\n",
      "Epoch 337/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0183\n",
      "Epoch 338/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0151\n",
      "Epoch 339/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0152\n",
      "Epoch 340/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0172\n",
      "Epoch 341/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0164\n",
      "Epoch 342/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0202\n",
      "Epoch 343/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0154\n",
      "Epoch 344/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0192\n",
      "Epoch 345/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0262\n",
      "Epoch 346/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0223\n",
      "Epoch 347/400\n",
      "1/1 [==============================] - 0s 890us/step - loss: 0.0184\n",
      "Epoch 348/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0151\n",
      "Epoch 349/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0172\n",
      "Epoch 350/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0175\n",
      "Epoch 351/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0148\n",
      "Epoch 352/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0147\n",
      "Epoch 353/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0166\n",
      "Epoch 354/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0154\n",
      "Epoch 355/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0193\n",
      "Epoch 356/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0151\n",
      "Epoch 357/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0180\n",
      "Epoch 358/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0243\n",
      "Epoch 359/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0167\n",
      "Epoch 360/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0174\n",
      "Epoch 361/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0145\n",
      "Epoch 362/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0144\n",
      "Epoch 363/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0146\n",
      "Epoch 364/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0162\n",
      "Epoch 365/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0155\n",
      "Epoch 366/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0190\n",
      "Epoch 367/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0146\n",
      "Epoch 368/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0194\n",
      "Epoch 369/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0238\n",
      "Epoch 370/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0170\n",
      "Epoch 371/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0213\n",
      "Epoch 372/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0161\n",
      "Epoch 373/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0167\n",
      "Epoch 374/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0142\n",
      "Epoch 375/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0141\n",
      "Epoch 376/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0143\n",
      "Epoch 377/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0156\n",
      "Epoch 378/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0151\n",
      "Epoch 379/400\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0183\n",
      "Epoch 380/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0142\n",
      "Epoch 381/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0193\n",
      "Epoch 382/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0223\n",
      "Epoch 383/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0174\n",
      "Epoch 384/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0203\n",
      "Epoch 385/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0140\n",
      "Epoch 386/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0139\n",
      "Epoch 387/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0139\n",
      "Epoch 388/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0139\n",
      "Epoch 389/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0156\n",
      "Epoch 390/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0149\n",
      "Epoch 391/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0271\n",
      "Epoch 392/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0191\n",
      "Epoch 393/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0185\n",
      "Epoch 394/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0154\n",
      "Epoch 395/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0140\n",
      "Epoch 396/400\n",
      "1/1 [==============================] - 0s 866us/step - loss: 0.0148\n",
      "Epoch 397/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0154\n",
      "Epoch 398/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0225\n",
      "Epoch 399/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0145\n",
      "Epoch 400/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0177\n",
      "best epoch =  387\n",
      "smallest loss = 0.013870060443878174\n"
     ]
    }
   ],
   "source": [
    "#After the compilation of the model, we’ll use the fit method with 500 epochs.\n",
    "#I started with epochs value of 100 and then tested the model after training. \n",
    "#The prediction was not that good. Then I modified the number of epochs to 200 and tested the model again. \n",
    "#Accuracy had improved slightly, but figured I’d give it one more try. Finally, at 500 epochs \n",
    "#I found acceptable prediction accuracy.\n",
    "\n",
    "#The fit method takes three parameters; namely, x, y, and number of epochs. \n",
    "#During model training, if all the batches of data are seen by the model once, \n",
    "#we say that one epoch has been completed.\n",
    "\n",
    "# Add an early stopping callback\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', \n",
    "    mode='min', \n",
    "    patience = 80, \n",
    "    restore_best_weights = True, \n",
    "    verbose=1)\n",
    "# Add a checkpoint where loss is minimum, and save that model\n",
    "mc = tf.keras.callbacks.ModelCheckpoint('best_model.SB', monitor='loss', \n",
    "                     mode='min',  verbose=1, save_best_only=True)\n",
    "\n",
    "historyData = model.fit(xarray,df.y3,epochs=400,callbacks=[es])\n",
    "\n",
    "loss_hist = historyData.history['loss']\n",
    "#The above line will return a dictionary, access it's info like this:\n",
    "best_epoch = np.argmin(historyData.history['loss']) + 1\n",
    "print ('best epoch = ', best_epoch)\n",
    "print('smallest loss =', np.min(loss_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.239287  ]\n",
      " [0.43670288]\n",
      " [0.8237637 ]]\n",
      "w01 =  1.239287 w02 =  0.43670288 w03 =  0.8237637\n",
      "[-0.19875675]\n",
      "b1 =  [-0.19875675]\n",
      "[[0.71184117]]\n",
      "w12 =  0.71184117\n",
      "[-0.16177657]\n",
      "b2 =  [-0.16177657]\n",
      "[[0.66874903]]\n",
      "w23 =  0.66874903\n",
      "[-0.00460504]\n",
      "b3 =  [-0.00460504]\n",
      "x01/20.2,  x02/14.5,   x03/308.0,  y3/32.4,  a3:\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "0.9900990099009901 0.896551724137931 1.009090909090909 0.9564519613592172 [[0.9587987]]\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "0.9900990099009901 1.0 1.0 0.9937964877627233 [[0.97673965]]\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "0.9900990099009901 1.0551724137931036 0.9935064935064936 0.9783648652819357 [[0.98566294]]\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1.0 0.896551724137931 1.009090909090909 0.954291534211907 [[0.9646399]]\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "0.9900990099009901 1.0 1.0 0.9999691367550383 [[0.97673965]]\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1.0 1.0551724137931036 0.9935064935064936 0.9691058917934631 [[0.9915041]]\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1.188118811881188 0.896551724137931 1.009090909090909 1.096571093484769 [[1.0756214]]\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1.7821782178217822 1.0 1.0003246753246755 1.4320545662170918 [[1.4441574]]\n",
      "  \n",
      "x01,  x02,   x03,  y3,  a3*32.4:\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "20.0 13.0 310.8 30.989043548038634 [[31.065079]]\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "20.0 14.5 308.0 32.19900620351223 [[31.646366]]\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "20.0 15.3 306.0 31.699021635134713 [[31.93548]]\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "20.2 13.0 310.8 30.919045708465788 [[31.254333]]\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "20.0 14.5 308.0 32.39900003086324 [[31.646366]]\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "20.2 15.3 306.0 31.3990308941082 [[32.124733]]\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "23.999999999999996 13.0 310.8 35.52890342890652 [[34.850136]]\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "36.0 14.5 308.1000000000001 46.398567945433776 [[46.7907]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#For results of training network:\n",
    "\n",
    "#keras.layer.get_weights() function retrieves weight values\n",
    "first_layer_weights = model.layers[0].get_weights()[0]\n",
    "w01 = first_layer_weights[0][0]\n",
    "w02 = first_layer_weights[1][0]\n",
    "w03 = first_layer_weights[2][0]\n",
    "first_layer_bias  = model.layers[0].get_weights()[1]\n",
    "b1 = first_layer_bias\n",
    "second_layer_weights = model.layers[1].get_weights()[0]\n",
    "w12 = second_layer_weights[0][0]\n",
    "second_layer_bias  = model.layers[1].get_weights()[1]\n",
    "b2 = second_layer_bias\n",
    "third_layer_weights = model.layers[2].get_weights()[0]\n",
    "w23 = third_layer_weights[0][0]\n",
    "third_layer_bias  = model.layers[2].get_weights()[1]\n",
    "b3 = third_layer_bias\n",
    "\n",
    "#print weights and biases\n",
    "print (first_layer_weights)\n",
    "print ('w01 = ', w01, 'w02 = ', w02, 'w03 = ', w03)\n",
    "print (first_layer_bias)\n",
    "print ('b1 = ', b1)\n",
    "print (second_layer_weights)\n",
    "print ('w12 = ', w12)\n",
    "print (second_layer_bias)\n",
    "print ('b2 = ', b2)\n",
    "print (third_layer_weights)\n",
    "print ('w23 = ', w23)\n",
    "print (third_layer_bias)\n",
    "print ('b3 = ', b3)\n",
    "\n",
    "#use model.predict() function to print model predictions for data conditions\n",
    "xarray = np.array(xdata)\n",
    "print ('x01/20.2,  x02/14.5,   x03/308.0,  y3/32.4,  a3:')\n",
    "test = []\n",
    "for i in range(0,8): \n",
    "    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n",
    "    testarray = np.array(test)\n",
    "    a3 = model.predict(testarray)\n",
    "    print (xarray[i][0], xarray[i][1], xarray[i][2], df.y3[i], a3)\n",
    "print('  ')\n",
    "print ('x01,  x02,   x03,  y3,  a3*32.4:')\n",
    "for i in range(0,8): \n",
    "    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n",
    "    testarray = np.array(test)\n",
    "    a3 = model.predict(testarray)\n",
    "    print (xarray[i][0]*20.2, xarray[i][1]*14.5, xarray[i][2]*308.0, df.y3[i]*32.4, a3*32.4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
