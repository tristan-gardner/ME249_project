{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.990099\n",
      "1    0.990099\n",
      "2    0.990099\n",
      "3    1.000000\n",
      "4    0.990099\n",
      "5    1.000000\n",
      "6    1.188119\n",
      "7    1.782178\n",
      "Name: x01, dtype: float64 0    0.896552\n",
      "1    1.000000\n",
      "2    1.055172\n",
      "3    0.896552\n",
      "4    1.000000\n",
      "5    1.055172\n",
      "6    0.896552\n",
      "7    1.000000\n",
      "Name: x02, dtype: float64 0    1.009091\n",
      "1    1.000000\n",
      "2    0.993506\n",
      "3    1.009091\n",
      "4    1.000000\n",
      "5    0.993506\n",
      "6    1.009091\n",
      "7    1.000325\n",
      "Name: x03, dtype: float64 0    0.956452\n",
      "1    0.993796\n",
      "2    0.978365\n",
      "3    0.954292\n",
      "4    0.999969\n",
      "5    0.969106\n",
      "6    1.096571\n",
      "7    1.432055\n",
      "Name: y3, dtype: float64\n",
      "[[0.9900990099009901, 0.896551724137931, 1.009090909090909], [0.9900990099009901, 1.0, 1.0], [0.9900990099009901, 1.0551724137931036, 0.9935064935064936], [1.0, 0.896551724137931, 1.009090909090909], [0.9900990099009901, 1.0, 1.0], [1.0, 1.0551724137931036, 0.9935064935064936], [1.188118811881188, 0.896551724137931, 1.009090909090909], [1.7821782178217822, 1.0, 1.0003246753246755]]\n",
      "[[0.99009901 0.89655172 1.00909091]\n",
      " [0.99009901 1.         1.        ]\n",
      " [0.99009901 1.05517241 0.99350649]\n",
      " [1.         0.89655172 1.00909091]\n",
      " [0.99009901 1.         1.        ]\n",
      " [1.         1.05517241 0.99350649]\n",
      " [1.18811881 0.89655172 1.00909091]\n",
      " [1.78217822 1.         1.00032468]]\n"
     ]
    }
   ],
   "source": [
    "'''>>>>> start CodeP2.2F24\n",
    "    V.P. Carey ME249, Fall 2024\n",
    "\n",
    "Intro to Neural Network Modeling \n",
    "Keras model for comparison with first principles model'''\n",
    "\n",
    "#import useful packages\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import keras.backend as kb\n",
    "import tensorflow as tf\n",
    "#the follwoing 2 lines are only needed for Mac OS machines\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "#raw data in dictionary form x01, x02, x03, y3\n",
    "my_dict = { \n",
    "    'x01' : [20., 20., 20., 20.2, 20., 20.2, 24.0, 36.],\n",
    "    'x02' : [13., 14.5, 15.3, 13., 14.5, 15.3, 13., 14.5],\n",
    "    'x03' : [310.8, 308.0, 306.0, 310.8, 308.0, 306.0, 310.8, 308.1],\n",
    "    'y3' : [30.99, 32.2, 31.7, 30.92, 32.4, 31.4, 35.53, 46.4]\n",
    "}\n",
    "#normalized inputs in array\n",
    "xdata = []\n",
    "xdata = [[20./20.2, 13.0/14.5, 310.8/308.0], [20./20.2, 14.5/14.5, 308.0/308.0]] \n",
    "xdata.append([20./20.2, 15.3/14.5, 306.0/308.0])\n",
    "xdata.append([20.2/20.2, 13.0/14.5, 310.8/308.0]) \n",
    "xdata.append([20./20.2, 14.5/14.5, 308.0/308.0]) \n",
    "xdata.append([20.2/20.2, 15.3/14.5, 306.0/308.0]) \n",
    "xdata.append([24./20.2, 13.0/14.5, 310.8/308.0]) \n",
    "xdata.append([36./20.2, 14.5/14.5, 308.1/308.0]) \n",
    "\n",
    "#data frame\n",
    "df = pd.DataFrame(my_dict)\n",
    "#devide by the median to normalize \n",
    "df.x01= df.x01/20.2\n",
    "df.x02= df.x02/14.5\n",
    "df.x03= df.x03/308.0\n",
    "#normalize output array\n",
    "df.y3= df.y3/32.401\n",
    "df.head\n",
    "print (df.x01, df.x02, df.x03, df.y3)\n",
    "\n",
    "xarray= np.array(xdata)\n",
    "print (xdata)\n",
    "print (xarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend \u001b[38;5;28;01mas\u001b[39;00m K\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#initialize weights with values between -0.2 and 1.2\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m initializer \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241m.\u001b[39minitializers\u001b[38;5;241m.\u001b[39mRandomUniform(minval\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.2\u001b[39m, maxval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.2\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# define three layer model with one neuron in each layer\u001b[39;00m\n\u001b[1;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m     15\u001b[0m     keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39mK\u001b[38;5;241m.\u001b[39melu, input_shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m3\u001b[39m],  kernel_initializer\u001b[38;5;241m=\u001b[39minitializer, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdense_one\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     16\u001b[0m     keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39mK\u001b[38;5;241m.\u001b[39melu,  kernel_initializer\u001b[38;5;241m=\u001b[39minitializer, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdense_two\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     17\u001b[0m     keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39mK\u001b[38;5;241m.\u001b[39melu,  kernel_initializer\u001b[38;5;241m=\u001b[39minitializer, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdense_three\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m   ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "\n",
    "#As seen below, we have created three dense layers each with just one neuron. \n",
    "#A dense layer is a layer in neural network that’s fully connected. \n",
    "#In other words, all the neurons in one layer are connected to all other neurons in the next layer.\n",
    "#In the first layer, we need to provide the input shape, which is 3 in this case. \n",
    "#The activation function we have chosen is ReLU, which stands for rectified linear unit.\n",
    "\n",
    "from keras import backend as K\n",
    "#initialize weights with values between -0.2 and 1.2\n",
    "initializer = keras.initializers.RandomUniform(minval= -0.2, maxval=1.2)\n",
    "\n",
    "# define three layer model with one neuron in each layer\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(1, activation=K.elu, input_shape=[3],  kernel_initializer=initializer, name=\"dense_one\"),\n",
    "    keras.layers.Dense(1, activation=K.elu,  kernel_initializer=initializer, name=\"dense_two\"),\n",
    "    keras.layers.Dense(1, activation=K.elu,  kernel_initializer=initializer, name=\"dense_three\")\n",
    "  ])\n",
    "model.summary()\n",
    "\n",
    "mult = 1.2\n",
    "\n",
    "#set starting values to those used in first principles model\n",
    "w01n =  1.23 * mult\n",
    "w02n =  0.40 * mult\n",
    "w03n =  0.70 * mult\n",
    "b1n =  -0.15 * mult\n",
    "w12n =  0.72 * mult\n",
    "b2n =  -0.12 * mult\n",
    "w23n =  0.7 * mult\n",
    "b3n =  0.01 * mult\n",
    "\n",
    "weights0 =  [[ w01n], [w02n], [ w03n]]\n",
    "w0array= np.array(weights0)\n",
    "print(np.shape(w0array))\n",
    "bias0 = [b1n]\n",
    "bias0array= np.array(bias0)\n",
    "L0=[]\n",
    "L0.append(w0array)\n",
    "L0.append(bias0array)\n",
    "model.layers[0].set_weights(L0) \n",
    "\n",
    "weights1 =  [[ w12n]]\n",
    "w1array= np.array(weights1)\n",
    "print(np.shape(w1array))\n",
    "bias1 = [b2n]\n",
    "bias1array= np.array(bias1)\n",
    "L1=[]\n",
    "L1.append(w1array)\n",
    "L1.append(bias1array)\n",
    "model.layers[1].set_weights(L1)\n",
    "\n",
    "weights2 =  [[ w23n]]\n",
    "w2array= np.array(weights2)\n",
    "print(np.shape(w2array))\n",
    "bias2 = [b3n]\n",
    "bias2array= np.array(bias2)\n",
    "L2=[]\n",
    "L2.append(w2array)\n",
    "L2.append(bias2array)\n",
    "model.layers[2].set_weights(L2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We’re using RMSprop as our optimizer here. RMSprop stands for Root Mean Square Propagation. \n",
    "#It’s one of the most popular gradient descent optimization algorithms for deep learning networks. \n",
    "#RMSprop is an optimizer that’s reliable and fast.\n",
    "#We’re compiling the mode using the model.compile function. The loss function used here \n",
    "#is mean absolute error. After the compilation of the model, we’ll use the fit method with 100 epochs.\n",
    "\n",
    "#Running model.fit successive times extends the calculation to addtional epochs.\n",
    "\n",
    "rms = keras.optimizers.RMSprop(0.0035)\n",
    "model.compile(loss='mean_absolute_error',optimizer=rms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0160\n",
      "Epoch 2/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0165\n",
      "Epoch 3/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0133\n",
      "Epoch 4/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0173\n",
      "Epoch 5/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0192\n",
      "Epoch 6/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0128\n",
      "Epoch 7/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0185\n",
      "Epoch 8/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0183\n",
      "Epoch 9/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0151\n",
      "Epoch 10/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0138\n",
      "Epoch 11/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0182\n",
      "Epoch 12/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0147\n",
      "Epoch 13/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0149\n",
      "Epoch 14/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0180\n",
      "Epoch 15/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0143\n",
      "Epoch 16/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0160\n",
      "Epoch 17/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0178\n",
      "Epoch 18/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0139\n",
      "Epoch 19/400\n",
      "1/1 [==============================] - 0s 945us/step - loss: 0.0174\n",
      "Epoch 20/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0227\n",
      "Epoch 21/400\n",
      "1/1 [==============================] - 0s 980us/step - loss: 0.0215\n",
      "Epoch 22/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0168\n",
      "Epoch 23/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0137\n",
      "Epoch 24/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0161\n",
      "Epoch 25/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0167\n",
      "Epoch 26/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0134\n",
      "Epoch 27/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0173\n",
      "Epoch 28/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0197\n",
      "Epoch 29/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0129\n",
      "Epoch 30/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0184\n",
      "Epoch 31/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0185\n",
      "Epoch 32/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0130\n",
      "Epoch 33/400\n",
      "1/1 [==============================] - 0s 965us/step - loss: 0.0129\n",
      "Epoch 34/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0188\n",
      "Epoch 35/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0188\n",
      "Epoch 36/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0129\n",
      "Epoch 37/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0129\n",
      "Epoch 38/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0190\n",
      "Epoch 39/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0189\n",
      "Epoch 40/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0129\n",
      "Epoch 41/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0129\n",
      "Epoch 42/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0192\n",
      "Epoch 43/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0190\n",
      "Epoch 44/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0129\n",
      "Epoch 45/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0159\n",
      "Epoch 46/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0128\n",
      "Epoch 47/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0162\n",
      "Epoch 48/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0128\n",
      "Epoch 49/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0164\n",
      "Epoch 50/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0128\n",
      "Epoch 51/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0167\n",
      "Epoch 52/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0128\n",
      "Epoch 53/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0170\n",
      "Epoch 54/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0127\n",
      "Epoch 55/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0172\n",
      "Epoch 56/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0126\n",
      "Epoch 57/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0132\n",
      "Epoch 58/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0226\n",
      "Epoch 59/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0226\n",
      "Epoch 60/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0193\n",
      "Epoch 61/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0195\n",
      "Epoch 62/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0126\n",
      "Epoch 63/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0162\n",
      "Epoch 64/400\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0126\n",
      "Epoch 65/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0165\n",
      "Epoch 66/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0126\n",
      "Epoch 67/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0131\n",
      "Epoch 68/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0218\n",
      "Epoch 69/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0188\n",
      "Epoch 70/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0192\n",
      "Epoch 71/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0126\n",
      "Epoch 72/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0199\n",
      "Epoch 73/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0182\n",
      "Epoch 74/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0149\n",
      "Epoch 75/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0145\n",
      "Epoch 76/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0181\n",
      "Epoch 77/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0145\n",
      "Epoch 78/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0156\n",
      "Epoch 79/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0179\n",
      "Epoch 80/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0141\n",
      "Epoch 81/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0168\n",
      "Epoch 82/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0228\n",
      "Epoch 83/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0157\n",
      "Epoch 84/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0171\n",
      "Epoch 85/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0137\n",
      "Epoch 86/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0168\n",
      "Epoch 87/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0207\n",
      "Epoch 88/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0160\n",
      "Epoch 89/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0166\n",
      "Epoch 90/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0134\n",
      "Epoch 91/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0172\n",
      "Epoch 92/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0193\n",
      "Epoch 93/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0129\n",
      "Epoch 94/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0184\n",
      "Epoch 95/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0184\n",
      "Epoch 96/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0151\n",
      "Epoch 97/400\n",
      "1/1 [==============================] - 0s 933us/step - loss: 0.0137\n",
      "Epoch 98/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0183\n",
      "Epoch 99/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0148\n",
      "Epoch 100/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0148\n",
      "Epoch 101/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0181\n",
      "Epoch 102/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0144\n",
      "Epoch 103/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0160\n",
      "Epoch 104/400\n",
      "1/1 [==============================] - 0s 982us/step - loss: 0.0178\n",
      "Epoch 105/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0140\n",
      "Epoch 106/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0172\n",
      "Epoch 107/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0229\n",
      "Epoch 108/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0213\n",
      "Epoch 109/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0168\n",
      "Epoch 110/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0137\n",
      "Epoch 111/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0160\n",
      "Epoch 112/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0167\n",
      "Epoch 113/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0134\n",
      "Epoch 114/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0171\n",
      "Epoch 115/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0198\n",
      "Epoch 116/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0164\n",
      "Epoch 117/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0163\n",
      "Epoch 118/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0131\n",
      "Epoch 119/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0177\n",
      "Epoch 120/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0188\n",
      "Epoch 121/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0129\n",
      "Epoch 122/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0131\n",
      "Epoch 123/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0180\n",
      "Epoch 124/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0191\n",
      "Epoch 125/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0128\n",
      "Epoch 126/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0131\n",
      "Epoch 127/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0183\n",
      "Epoch 128/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0192\n",
      "Epoch 129/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0128\n",
      "Epoch 130/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0131\n",
      "Epoch 131/400\n",
      "1/1 [==============================] - 0s 992us/step - loss: 0.0185\n",
      "Epoch 132/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0193\n",
      "Epoch 133/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0128\n",
      "Epoch 134/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0131\n",
      "Epoch 135/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0187\n",
      "Epoch 136/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0194\n",
      "Epoch 137/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0127\n",
      "Epoch 138/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0131\n",
      "Epoch 139/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0188\n",
      "Epoch 140/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0194\n",
      "Epoch 141/400\n",
      "1/1 [==============================] - 0s 985us/step - loss: 0.0127\n",
      "Epoch 142/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0131\n",
      "Epoch 143/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0190\n",
      "Epoch 144/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0193\n",
      "Epoch 145/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0126\n",
      "Epoch 146/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0130Restoring model weights from the end of the best epoch: 66.\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0130\n",
      "Epoch 146: early stopping\n",
      "best epoch =  66\n",
      "smallest loss = 0.012552857398986816\n"
     ]
    }
   ],
   "source": [
    "#After the compilation of the model, we’ll use the fit method with 500 epochs.\n",
    "#I started with epochs value of 100 and then tested the model after training. \n",
    "#The prediction was not that good. Then I modified the number of epochs to 200 and tested the model again. \n",
    "#Accuracy had improved slightly, but figured I’d give it one more try. Finally, at 500 epochs \n",
    "#I found acceptable prediction accuracy.\n",
    "\n",
    "#The fit method takes three parameters; namely, x, y, and number of epochs. \n",
    "#During model training, if all the batches of data are seen by the model once, \n",
    "#we say that one epoch has been completed.\n",
    "\n",
    "# Add an early stopping callback\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', \n",
    "    mode='min', \n",
    "    patience = 80, \n",
    "    restore_best_weights = True, \n",
    "    verbose=1)\n",
    "# Add a checkpoint where loss is minimum, and save that model\n",
    "mc = tf.keras.callbacks.ModelCheckpoint('best_model.SB', monitor='loss', \n",
    "                     mode='min',  verbose=1, save_best_only=True)\n",
    "\n",
    "historyData = model.fit(xarray,df.y3,epochs=400,callbacks=[es])\n",
    "\n",
    "loss_hist = historyData.history['loss']\n",
    "#The above line will return a dictionary, access it's info like this:\n",
    "best_epoch = np.argmin(historyData.history['loss']) + 1\n",
    "print ('best epoch = ', best_epoch)\n",
    "print('smallest loss =', np.min(loss_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.2089251 ]\n",
      " [0.37135276]\n",
      " [0.70917636]]\n",
      "w01 =  1.2089251 w02 =  0.37135276 w03 =  0.70917636\n",
      "[-0.14463164]\n",
      "b1 =  [-0.14463164]\n",
      "[[0.70464283]]\n",
      "w12 =  0.70464283\n",
      "[-0.11115543]\n",
      "b2 =  [-0.11115543]\n",
      "[[0.68103135]]\n",
      "w23 =  0.68103135\n",
      "[0.02243863]\n",
      "b3 =  [0.02243863]\n",
      "x01/20.2,  x02/14.5,   x03/308.0,  y3/32.4,  a3:\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "0.9900990099009901 0.896551724137931 1.009090909090909 0.9564519613592172 [[0.95491886]]\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "0.9900990099009901 1.0 1.0 0.9937964877627233 [[0.9702601]]\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "0.9900990099009901 1.0551724137931036 0.9935064935064936 0.9783648652819357 [[0.97788227]]\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1.0 0.896551724137931 1.009090909090909 0.954291534211907 [[0.9606629]]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "0.9900990099009901 1.0 1.0 0.9999691367550383 [[0.9702601]]\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1.0 1.0551724137931036 0.9935064935064936 0.9691058917934631 [[0.9836263]]\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1.188118811881188 0.896551724137931 1.009090909090909 1.096571093484769 [[1.0697987]]\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1.7821782178217822 1.0 1.0003246753246755 1.4320545662170918 [[1.4298903]]\n",
      "  \n",
      "x01,  x02,   x03,  y3,  a3*32.4:\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "20.0 13.0 310.8 30.989043548038634 [[30.939373]]\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "20.0 14.5 308.0 32.19900620351223 [[31.436428]]\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "20.0 15.3 306.0 31.699021635134713 [[31.683388]]\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "20.2 13.0 310.8 30.919045708465788 [[31.125479]]\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "20.0 14.5 308.0 32.39900003086324 [[31.436428]]\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "20.2 15.3 306.0 31.3990308941082 [[31.869493]]\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "23.999999999999996 13.0 310.8 35.52890342890652 [[34.66148]]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "36.0 14.5 308.1000000000001 46.398567945433776 [[46.328445]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#For results of training network:\n",
    "\n",
    "#keras.layer.get_weights() function retrieves weight values\n",
    "first_layer_weights = model.layers[0].get_weights()[0]\n",
    "w01 = first_layer_weights[0][0]\n",
    "w02 = first_layer_weights[1][0]\n",
    "w03 = first_layer_weights[2][0]\n",
    "first_layer_bias  = model.layers[0].get_weights()[1]\n",
    "b1 = first_layer_bias\n",
    "second_layer_weights = model.layers[1].get_weights()[0]\n",
    "w12 = second_layer_weights[0][0]\n",
    "second_layer_bias  = model.layers[1].get_weights()[1]\n",
    "b2 = second_layer_bias\n",
    "third_layer_weights = model.layers[2].get_weights()[0]\n",
    "w23 = third_layer_weights[0][0]\n",
    "third_layer_bias  = model.layers[2].get_weights()[1]\n",
    "b3 = third_layer_bias\n",
    "\n",
    "#print weights and biases\n",
    "print (first_layer_weights)\n",
    "print ('w01 = ', w01, 'w02 = ', w02, 'w03 = ', w03)\n",
    "print (first_layer_bias)\n",
    "print ('b1 = ', b1)\n",
    "print (second_layer_weights)\n",
    "print ('w12 = ', w12)\n",
    "print (second_layer_bias)\n",
    "print ('b2 = ', b2)\n",
    "print (third_layer_weights)\n",
    "print ('w23 = ', w23)\n",
    "print (third_layer_bias)\n",
    "print ('b3 = ', b3)\n",
    "\n",
    "#use model.predict() function to print model predictions for data conditions\n",
    "xarray = np.array(xdata)\n",
    "print ('x01/20.2,  x02/14.5,   x03/308.0,  y3/32.4,  a3:')\n",
    "test = []\n",
    "for i in range(0,8): \n",
    "    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n",
    "    testarray = np.array(test)\n",
    "    a3 = model.predict(testarray)\n",
    "    print (xarray[i][0], xarray[i][1], xarray[i][2], df.y3[i], a3)\n",
    "print('  ')\n",
    "print ('x01,  x02,   x03,  y3,  a3*32.4:')\n",
    "for i in range(0,8): \n",
    "    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n",
    "    testarray = np.array(test)\n",
    "    a3 = model.predict(testarray)\n",
    "    print (xarray[i][0]*20.2, xarray[i][1]*14.5, xarray[i][2]*308.0, df.y3[i]*32.4, a3*32.4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
